---
layout: default
title: Etienne Lasalle, Research
---
<div class="blurb">
  <h1 class="title1">Research</h1>
	<p class="normalsize"> My research focuses on <b>statistics related to graph-structured data</b>. <br>
    During my thesis, I developed and studied tools for <b>multi-scale graph comparisons</b> 
    based on heat diffusion and topological data analysis.
    These tools come with statistical guarantees that ensure the asymptotic validity of two-sample tests.
    Implementing these methods allowed me to apply them to practical problems, 
    particularly in machine learning and neural network classifiers.<br>
    Currently, I am exploring various aspects of <b>graph data compression</b>. 
    This includes work on graph inference through a compressive learning method and, more recently, 
    investigating methods to accelerate community detection algorithms using coarsening techniques.
	</p>
<div class="pagenav">
  <a class="button button_pagenav" href="#articles">Articles</a>
  <a class="button button_pagenav" href="#talks"   >Talks   </a>
  <a class="button button_pagenav" href="#material"   >Material   </a>
  <a class="button button_pagenav" href="#others"  >Others  </a>
</div>

  <h2 id="articles" class="title2">Articles</h2>

  <table class="normalsize">
    <tr>
    <td class="nopadding top" width=150>
    <a href="https://hal.science/hal-04837207v1/document">
    <img class="noradius" src="fig/pasco.gif" width=150 height=100></a>
    </td>
    <td class="nopadding top">
    <b>PASCO (PArallel Structured COarsening): an overlay to speed up graph clustering algorithms,</b> 2024. <br>
    With R. Vaudaine, T. Vayer, P. Borgnat, R. Gribonval, P. Gonçalves, M. Karsai<br>
    <!-- [<a href="https://arxiv.org/pdf/2311.04673.pdf">arxiv</a>] -->
        <button class="showhide art_arbstr_4" onclick="switchDisplayByClassName('art_arbstr_4')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
        <button class="showhide art_arbstr_4" onclick="switchDisplayByClassName('art_arbstr_4')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
        <p class="abstracttext art_arbstr_4" style="display : none;">
          Clustering the nodes of a graph is a cornerstone of graph analysis and has been extensively studied. However, some popular methods are not suitable for very large graphs: e.g., spectral clustering requires the computation of the spectral decomposition of the Laplacian matrix, which is not applicable for large graphs with a large number of communities. This work introduces PASCO, an overlay that accelerates clustering algorithms. Our method consists of three steps: 
          1- We compute several independent small graphs representing the input graph by applying an efficient and structure-preserving coarsening algorithm. 
          2- A clustering algorithm is run in parallel onto each small graph and provides several partitions of the initial graph. 
          3- These partitions are aligned and combined with an optimal transport method to output the final partition. 
          The PASCO framework is based on two key contributions: a novel global algorithm structure designed to enable parallelization and a fast, empirically validated graph coarsening algorithm that preserves structural properties.
          We demonstrate the strong performance of PASCO in terms of computational efficiency, structural preservation, and output partition quality, evaluated on both synthetic and real-world graph datasets.
        </p>
    </td>
    </tr>

    <tr>
    <td class="nopadding top" width=150>
    <a href="https://arxiv.org/pdf/2311.04673.pdf">
    <img class="noradius" src="fig/compr_recov_theta.PNG" width=150 height=100></a>
    </td>
    <td class="nopadding top">
    <b>Compressive Recovery of Sparse Precision Matrices,</b> 2023. <br>
    With T. Vayer, R. Gribonval, P. Gonçalves<br>
    [<a href="https://arxiv.org/pdf/2311.04673.pdf">arxiv</a>]
        <button class="showhide art_arbstr_3" onclick="switchDisplayByClassName('art_arbstr_3')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
        <button class="showhide art_arbstr_3" onclick="switchDisplayByClassName('art_arbstr_3')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
        <p class="abstracttext art_arbstr_3" style="display : none;">
          We consider the problem of learning a graph modeling the statistical relations of the \(d\) variables of a dataset with \(n\) samples \(\mathbf{X} \in \mathbb{R}^{n \times d}\).
          Standard approaches amount to searching for a precision matrix \(\boldsymbol\Theta\) representative of a Gaussian graphical model that adequately explains the data.
          However, most maximum likelihood-based estimators usually require storing the \(d^{2}\) values of the empirical covariance matrix, which can become prohibitive in a high-dimensional setting. <br>
          In this work, we adopt a ‘‘compressive'' viewpoint and aim to estimate a sparse \(\boldsymbol\Theta\) from a <em>sketch</em> of the data, <em>i.e.,</em> a low-dimensional vector of size \(m \ll d^{2}\) carefully designed from \(\mathbf{X}\) using nonlinear random features.
          Under certain assumptions on the spectrum of \(\boldsymbol\Theta\) (or its condition number), we show that it is possible to estimate it from a sketch of size \(m=\Omega\left((d+2k)\log(d)\right)\) where \(k\) is the maximal number of edges of the underlying graph.
          These information-theoretic guarantees are inspired by compressed sensing theory and involve restricted isometry properties and instance optimal decoders.
          We investigate the possibility of achieving practical recovery with an iterative algorithm based on the graphical lasso, viewed as a specific denoiser.
          We compare our approach and graphical lasso on synthetic datasets, demonstrating its favorable performance even when the dataset is compressed.
        </p>
    </td>
    </tr>

    <tr>
    <td class="nopadding top" width=150>
    <a href="https://arxiv.org/pdf/2303.04752.pdf">
    <img class="noradius" src="fig/paquet20000.jpg" width=150 height=100></a>
    </td>
    <td class="nopadding top">
    <b>Eve, Adam and the Preferential Attachment Tree,</b> 2023. <br>
    With A. Contat, N. Curien, P. Lacroix and V. Rivoirard <br>
    Published in <i>Probability Theory and Related Fields</i>. <br>
    [<a href="https://link.springer.com/article/10.1007/s00440-023-01253-1">journal</a>] [<a href="https://arxiv.org/pdf/2303.04752.pdf">arxiv</a>]
        <button class="showhide art_arbstr_2" onclick="switchDisplayByClassName('art_arbstr_2')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
        <button class="showhide art_arbstr_2" onclick="switchDisplayByClassName('art_arbstr_2')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
        <p class="abstracttext art_arbstr_2" style="display : none;">
          We consider the problem of finding the initial vertex (Adam) in a Barabasi--Albert tree process  \( (\mathcal{T}(n) : n \geq 1) \) at large times.
          More precisely, given \( \varepsilon>0 \), one wants to output a subset \(\mathcal{P}_{ \varepsilon}(n) \) of  vertices of  \(\mathcal{T}(n)\) so that the initial vertex belongs  to \(\mathcal{P}_ \varepsilon(n)\) with probability at least \(1- \varepsilon\) when \(n\) is large.
          It has been shown by Bubeck, Devroye & Lugosi (2017), refined later by Banerjee & Huang (2023),  that one needs to output at least \(\varepsilon^{-1 + o(1)}\)  and at most \(\varepsilon^{-2 + o(1)}\) vertices to succeed.
          We prove that the exponent in the lower bound is sharp and the key idea is that Adam is either a ``large degree" vertex or is a neighbor of a ``large degree" vertex (Eve).
        </p>
    </td>
    </tr>

			<tr>
			<td class="nopadding top" width=150>
			<a href="https://arxiv.org/pdf/2109.13213.pdf">
			<img class="noradius" src="fig/animated_slow.gif" width=150 height=100></a>
			</td>
			<td class="nopadding top">
			<b>Heat diffusion distance processes: a statistically founded method to analyze graph data sets,</b> 2023. <br>
      Published in <i>J Appl. and Comput. Topology</i> (SI : <i>Data Science on Graphs</i>). <br>
			[<a href="https://rdcu.be/dctmr">journal</a>] [<a href="https://arxiv.org/pdf/2109.13213.pdf">arxiv</a>]
          <button class="showhide art_arbstr_1" onclick="switchDisplayByClassName('art_arbstr_1')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
          <button class="showhide art_arbstr_1" onclick="switchDisplayByClassName('art_arbstr_1')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
          <p class="abstracttext art_arbstr_1" style="display : none;">
            We propose two multiscale comparisons of graphs using heat diffusion, allowing to compare graphs without node correspondence or even with different sizes.
            These multiscale comparisons lead to the definition of Lipschitz-continuous empirical processes indexed by a real parameter.
            The statistical properties of empirical means of such processes are studied in the general case.
            Under mild assumptions, we prove a functional Central Limit Theorem, as well as a Gaussian approximation with a rate depending only on the sample size.
            Once applied to our processes, these results allow to analyze data sets of pairs of graphs.
            We design consistent confidence bands around empirical means and consistent two-sample tests, using bootstrap methods.
            Their performances are evaluated by simulations on synthetic data sets.
          </p>
			</td>
			</tr>
  </table>




  <h2 id="talks" class="title2">Talks</h2>
  <ul class="normalsize">
    <li>November, 2024 - Rennes - Malt seminar. <br>
      <b>Statistical comparison of graph-structured data and its application to distribution shift detection</b>.
      <button class="showhide abstract19" onclick="switchDisplayByClassName('abstract19')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
      <button class="showhide abstract19" onclick="switchDisplayByClassName('abstract19')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
      <p class="abstracttext abstract19" style="display : none;">
        When working with weighted graphs, one can interpret weights as the thermal conductivity of edges. 
        Given initial conditions, one can use the way heat diffuses to compare graphs. 
        Choosing relevant and informative diffusion times is often essential and challenging. 
        To circumvent this issue, we choose to take into account the whole diffusion process. 
        We define real-valued processes indexed by [0, T] for some T > 0, representing the comparisons for all diffusion times. 
        Using tools from topological data analysis, we are able to compare graphs with unknown node correspondence or even graphs with different sizes.
        In this talk, I will introduce these processes and present their statistical properties. 
        From these results, we will see how we can construct consistent two-sample tests. 
        Then, I will present some applications for the detection of distribution shifts in the context of neural networks by using activation graphs.
      </p>
    </li>
    <br>

    <li>September, 2024 - Nantes - <a href="https://sims.ls2n.fr/">SIMS</a> seminar. <br>
      <b>Compressive Recovery of Sparse Precision Matrices</b>. <br>
      <button class="showhide abstract18" onclick="switchDisplayByClassName('abstract18')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
      <button class="showhide abstract18" onclick="switchDisplayByClassName('abstract18')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
      <p class="abstracttext abstract18" style="display : none;">
        We consider the problem of learning a graph modeling the statistical relations of the \(d\) variables of a dataset with \(n\) samples.
        Standard approaches amount to searching for a precision matrix representative of a Gaussian graphical model that adequately explains the data.
        However, most maximum likelihood-based estimators usually require storing the \(d^2\) values of the empirical covariance matrix, which can become prohibitive in a high-dimensional setting. <br>
        In this talk, we adopt a “compressive” viewpoint and aim to estimate a sparse precision matrix from a sketch of the data, i.e., a low-dimensional vector of size \(m\ll d^2\) carefully designed from the data using nonlinear random features (e.g., rank-one projections).
        Under certain spectral assumptions, we show that it is possible to recover the precision matrix from a sketch of size \(m=\Omega\left((d+2k)\log(d)\right)\) where \(k\) is the maximal number of edges of the underlying graph.
        These information-theoretic guarantees are inspired by the compressed sensing theory.
        We investigate the possibility of achieving practical recovery with an iterative algorithm based on the graphical lasso, viewed as a specific denoiser.
        We compare our approach and the graphical lasso on synthetic datasets, demonstrating its favorable performance even when the dataset is compressed. <br>
        Joint work with : Titouan Vayer, Rémi Gribonval and Paulo Gonçalves.
      </p>
    </li>
    <br>

    <li>June, 2024 - Copenhagen - <a href="https://popnets.wordpress.com/">Popnets Workshop</a>. <br>
      <b>Statistical comparison of graph-structured data and its application to distribution shift detection</b>.
      <button class="showhide abstract17" onclick="switchDisplayByClassName('abstract17')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
      <button class="showhide abstract17" onclick="switchDisplayByClassName('abstract17')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
      <p class="abstracttext abstract17" style="display : none;">
        When working with weighted graphs, one can interpret weights as the thermal conductivity of edges. 
        Given initial conditions, one can use the way heat diffuses to compare graphs. 
        Choosing relevant and informative diffusion times is often essential and challenging. 
        To circumvent this issue, we choose to take into account the whole diffusion process. 
        We define real-valued processes indexed by [0, T] for some T > 0, representing the comparisons for all diffusion times. 
        Using tools from topological data analysis, we are able to compare graphs with unknown node correspondence or even graphs with different sizes.
        In this talk, I will introduce these processes and present their statistical properties. 
        From these results, we will see how we can construct consistent two-sample tests. 
        Then, I will present some applications for the detection of distribution shifts in the context of neural networks by using activation graphs.
      </p>
    </li>
    <br>

    <li>May, 2024 - Bordeaux - <a href="https://jds2024.sciencesconf.org/">Journées de Statistiques</a>. <br>
      <b>Compressive Recovery of Sparse Precision Matrices</b>. - [<a href="slides/lasalle_jds_2024_compressed.pdf">slides</a>] <br>
      <button class="showhide abstract16" onclick="switchDisplayByClassName('abstract16')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
      <button class="showhide abstract16" onclick="switchDisplayByClassName('abstract16')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
      <p class="abstracttext abstract16" style="display : none;">
        We consider the problem of learning a graph modeling the statistical relations of the \(d\) variables of a dataset with \(n\) samples.
        Standard approaches amount to searching for a precision matrix representative of a Gaussian graphical model that adequately explains the data.
        However, most maximum likelihood-based estimators usually require storing the \(d^2\) values of the empirical covariance matrix, which can become prohibitive in a high-dimensional setting. <br>
        In this talk, we adopt a “compressive” viewpoint and aim to estimate a sparse precision matrix from a sketch of the data, i.e., a low-dimensional vector of size \(m\ll d^2\) carefully designed from the data using nonlinear random features (e.g., rank-one projections).
        Under certain spectral assumptions, we show that it is possible to recover the precision matrix from a sketch of size \(m=\Omega\left((d+2k)\log(d)\right)\) where \(k\) is the maximal number of edges of the underlying graph.
        These information-theoretic guarantees are inspired by the compressed sensing theory.
        We investigate the possibility of achieving practical recovery with an iterative algorithm based on the graphical lasso, viewed as a specific denoiser.
        We compare our approach and the graphical lasso on synthetic datasets, demonstrating its favorable performance even when the dataset is compressed. <br>
        Joint work with : Titouan Vayer, Rémi Gribonval and Paulo Gonçalves.
      </p>
    </li>
    <br>

  <li>November, 2023 - ENS de Lyon - <a href="https://gdr-mia.math.cnrs.fr/events/dimreduc/">RT MIA Workshop</a>: Dimension Reduction for Learning and Visualization. <br>
    <b>Compressive Recovery of Sparse Precision Matrices</b>. <br>
    <button class="showhide abstract15" onclick="switchDisplayByClassName('abstract15')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
    <button class="showhide abstract15" onclick="switchDisplayByClassName('abstract15')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
    <p class="abstracttext abstract15" style="display : none;">
      We consider the problem of learning a graph modeling the statistical relations of the \(d\) variables of a dataset with \(n\) samples.
      Standard approaches amount to searching for a precision matrix representative of a Gaussian graphical model that adequately explains the data.
      However, most maximum likelihood-based estimators usually require storing the \(d^2\) values of the empirical covariance matrix, which can become prohibitive in a high-dimensional setting. <br>
      In this talk, we adopt a “compressive” viewpoint and aim to estimate a sparse precision matrix from a sketch of the data, i.e., a low-dimensional vector of size \(m\ll d^2\) carefully designed from the data using nonlinear random features (e.g., rank-one projections).
      Under certain spectral assumptions, we show that it is possible to recover the precision matrix from a sketch of size \(m=\Omega\left((d+2k)\log(d)\right)\) where \(k\) is the maximal number of edges of the underlying graph.
      These information-theoretic guarantees are inspired by the compressed sensing theory.
      We investigate the possibility of achieving practical recovery with an iterative algorithm based on the graphical lasso, viewed as a specific denoiser.
      We compare our approach and the graphical lasso on synthetic datasets, demonstrating its favorable performance even when the dataset is compressed. <br>
      Joint work with : Titouan Vayer, Rémi Gribonval and Paulo Gonçalves.
    </p>
  </li>
  <br>
  <li>May, 2023 - LJK Grenoble - Seminar of the <a href="https://www-ljk.imag.fr/spip.php?article6">DATA department</a>. <br>
    <b>Statistical comparison of graph-structured data and its application to distribution shift detection</b>. <br>
    <button class="showhide abstract14" onclick="switchDisplayByClassName('abstract14')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
    <button class="showhide abstract14" onclick="switchDisplayByClassName('abstract14')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
    <p class="abstracttext abstract14" style="display : none;">
      When working with weighted graphs, one can interpret weights as the thermal conductivity of edges. Given initial conditions, one can use the way heat diffuses to compare graphs. Choosing relevant and informative diffusion times is often essential and challenging. To circumvent this issue, we choose to take into account the whole diffusion process. We define real-valued processes indexed by [0, T] for some T > 0, representing the comparisons for all diffusion times. Using tools from topological data analysis, we are able to compare graphs with unknown node correspondence or even graphs with different sizes. <br>
      In this talk, I will introduce these processes and present their statistical properties. From these results, we will see how we can construct consistent two-sample tests. Then I will present some applications to the detection of distribution shifts in the context of neural networks by using activation graphs.
    </p>
  </li>
  <br>
  <li> January, 2023 - Campus Agro Paris-Saclay - Meeting of the <a href="http://cmatias.perso.math.cnrs.fr/ANR_EcoNet.html">EcoNet</a> project. <br>
    <b>Statistical comparison of graph structured data</b>. <br>
    <button class="showhide abstract13" onclick="switchDisplayByClassName('abstract13')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
    <button class="showhide abstract13" onclick="switchDisplayByClassName('abstract13')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
    <p class="abstracttext abstract13" style="display : none;">
      Not provided.
    </p>
    </li>
    <br>
  <li>January, 2023 - CIRM - <a href="https://conferences.cirm-math.fr/3021.html">Workshop on Random Geometry</a>.<br>
    <b>Finding Adam in the nearest-neighbor tree</b>.<br>
      <button class="showhide abstract12" onclick="switchDisplayByClassName('abstract12')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
      <button class="showhide abstract12" onclick="switchDisplayByClassName('abstract12')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
      <p class="abstracttext abstract12" style="display : none;">
        In this talk, I present the problem of finding the root in the online nearest-neighbor tree (NNT) model.  Consider a probability distribution on some metric space.
        Here we will restrict ourselves to the uniform distribution on the unit circle with the Euclidean distance.
        The random tree is constructed recursively by linking each new node to its nearest neighbor. <br>

        Assume that we only observe the structure of such a large tree. That is, we only have access to the connectivity and neither the vertex positions in the metric space nor the vertex labels are available.
        Can we find a set $S$ of vertices with a reasonable size (i.e., independent of the tree size) that contain the root with high probability.
        I will explain how we can obtain such set by using a notion of centrality, and present the similarities and differences with the uniform random recursive tree (uRRT) model.
    </p>
    </li>
    <br>
  <li>November, 2022 - Orsay - Working group of the Probability-Statistics team.<br>
  <b>Testing between the stochastic block model and the Erdos-Renyi model</b>. [<a href="https://link.springer.com/content/pdf/10.1007/s00440-014-0576-6.pdf">article</a>].<br>
  With Leonardo Martins-Bianco and <a href="https://www.imo.universite-paris-saclay.fr/~zacharie.naulet/">Zacharie Naulet</a>.<br>
    <button class="showhide abstrac11" onclick="switchDisplayByClassName('abstract11')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
    <button class="showhide abstract11" onclick="switchDisplayByClassName('abstract11')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
    <p class="abstracttext abstract11" style="display : none;">
      In a first part, we talk about the detection of the SBM vs ER model, in the sparse case, in the regime where detection is possible. We show that by counting cycles we can distinguish between the two models.
      Then, we talk about finding the communities in an SBM graph. We present the spectral clustering algorithm and show that it is efficient in the dense case.
  </p>
  </li>
  <br>
  <li>June, 2022 - Lyon - <a href="https://jds22.sciencesconf.org/">Journées de Statistiques</a> de la <a href="https://www.sfds.asso.fr/">SFdS</a>.<br>
  <b>Analyse statistique de graphes, via des processus de diffusion de la chaleur.</b><br>
    <button class="showhide abstract10" onclick="switchDisplayByClassName('abstract10')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
    <button class="showhide abstract10" onclick="switchDisplayByClassName('abstract10')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
    <p class="abstracttext abstract10" style="display : none;">
      Ce travail porte sur la comparaison de données de graphes, potentiellement pondérés et de tailles différentes.
      Lorsqu’on travaille avec des graphes pondérés, on peut interpréter les poids des arêtes comme des conductivités thermiques.
      Dès lors, on peut comparer les graphes en comparant leur répartition de chaleur après un temps de diffusion t.
      Ce paramètre d’échelle t doit être minutieusement choisi pour s’assurer des comparaisons pertinentes.
      A l’opposé de précédents travaux considérant un temps de diffusion fixé arbitrairement ou choisi à partir des données, on propose de prendre en compte tout le processus de diffusion.
      Pour cela, on définit des processus à valeurs réelles indexés par tous les temps de diffusion dans (0,T), en concaténant les comparaisons faites aux différentes échelles.
      Dans cet exposé, nous commencerons par présenter ces processus de comparaison de graphes et leurs propriétés statistiques.
      Puis, nous montrerons comment en dériver des tests à deux échantillons consistants.
      Nous présenterons quelques applications sur des jeux de données synthétiques et réels. On s’intéressera notamment aux données provenant de graphes d’activations de réseaux de neurones.
  </p>
  </li>
  <br>
  <li>June, 2022 - Lyon - <a href="https://www.ens-lyon.fr/PHYSIQUE/seminars/machine-learning-and-signal-processing">Machine Learning and Signal Processing Seminar</a>.<br>
  <b>Heat diffusion distance processes for graphs and their application to distribution shift detection.</b><br>
    <button class="showhide abstract9" onclick="switchDisplayByClassName('abstract9')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
    <button class="showhide abstract9" onclick="switchDisplayByClassName('abstract9')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
    <p class="abstracttext abstract9" style="display : none;">
      When working with weighted graphs, one can interpret weights as the thermal conductivity of edges. Given initial conditions, one can use the way heat diffuses to compare graphs. Choosing relevant and informative diffusion times is often essential and challenging. To circumvent this issue, we choose to take into account the whole diffusion process. We define real-valued processes indexed by [0, T] for some T > 0, representing the comparisons for all diffusion times. Using tools from topological data analysis, we are able to compare graphs with unknown node correspondence or even graphs with different sizes. <br>
      In this talk, I will introduce these processes and present their statistical properties. From these results, we will see how we can construct consistent two-sample tests. Then I will present some applications to the detection of distribution shifts in the context of neural networks by using activation graphs.
  </p>
  </li>
  <br>
  <li>June, 2022 - Orsay - <a href="https://www.inria.fr/fr/celeste">Celeste</a> seminar.<br>
  <b>Heat diffusion distance processes for graphs and their application to distribution shift detection.</b><br>
    <button class="showhide abstract8" onclick="switchDisplayByClassName('abstract8')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
    <button class="showhide abstract8" onclick="switchDisplayByClassName('abstract8')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
    <p class="abstracttext abstract8" style="display : none;">
      When working with weighted graphs, one can interpret weights as the thermal conductivity of edges. Given initial conditions, one can use the way heat diffuses to compare graphs. Choosing relevant and informative diffusion times is often essential and challenging. To circumvent this issue, we choose to take into account the whole diffusion process. We define real-valued processes indexed by [0, T] for some T > 0, representing the comparisons for all diffusion times. Using tools from topological data analysis, we are able to compare graphs with unknown node correspondence or even graphs with different sizes. <br>
      In this talk, I will introduce these processes and present their statistical properties. From these results, we will see how we can construct consistent two-sample tests. Then I will present some applications to the detection of distribution shifts in the context of neural networks by using activation graphs.
  </p>
  </li>
  <br>
  <li>May, 2022 - Orsay - Working group of the Probability-Statistics team.<br>
  Presentation of <b>Density estimation from unweighted k-nearest neighbor graphs: a roadmap</b> [<a href="https://proceedings.neurips.cc/paper/2013/file/eae27d77ca20db309e056e3d2dcd7d69-Paper.pdf">article</a>].<br>
  With <a href="https://sites.google.com/view/acontat/">Alice Contat</a> and <a href="https://www.imo.universite-paris-saclay.fr/~curien/index.html">Nicolas Curien</a>.<br>
    <button class="showhide abstract7" onclick="switchDisplayByClassName('abstract7')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
    <button class="showhide abstract7" onclick="switchDisplayByClassName('abstract7')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
    <p class="abstracttext abstract7" style="display : none;">
      Consider a density f on R^d, and some i.i.d. sample of size n following this density. Construct the oriented k-nearest neighbor graph. Can we, only from the graph structure (i.e. forgetting the embedding in R^d), recover the density.
      We study the 1D case in details. In particular, we explained why the order of magnitude of k needs to be greater than n^(2/3). We proposed some potential improvements of the proposed method for d=1.
  </p>
  </li>
  <br>
  <li>May, 2022 - Porquerolles - Datashape seminar.<br>
  <b>Detecting distribution shifts using activation graphs from neural networks.</b><br>
    <button class="showhide abstract6" onclick="switchDisplayByClassName('abstract6')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
    <button class="showhide abstract6" onclick="switchDisplayByClassName('abstract6')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
    <p class="abstracttext abstract6" style="display : none;">
      The abstract is not available yet.
  </p>
  </li>
  <br>
  <li>Mar, 2022 - Orsay - Working group of the Probability-Statistics team.<br>
Presentation of <b>Identifying the deviator</b> [<a href="https://arxiv.org/abs/2203.03744">arxiv</a>].<br>
  <button class="showhide abstract5" onclick="switchDisplayByClassName('abstract5')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
  <button class="showhide abstract5" onclick="switchDisplayByClassName('abstract5')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
  <p class="abstracttext abstract5" style="display : none;">
    Alice and Bob are playing a game. They construct a binary sequence by alternatively anouncing 0 or 1. Imagine that a rule says that they should announce their digit by following a Bernoulli distribution of parameter 1/2, independently of the past of the sequence.
    If they both follow the rule, the sequence will verify several properties. For example,  A : the proportion of 1s in the sequence tends to 1/2 ; B : the associated random walk (where we make a +1 step when a player announced 1 and a -1 step when a player announced 0) will cross 0 infinitely many times.
    Assuming that A or B is not observed, and that only one of the players is cheating, I present the article's results that show how we can identify the cheater almost surely, given the whole binary sequence. The article actually tackles a more general problem, but I only focus on these two examples.
</p>
</li>
<br>
    <li>Dec, 2021 - Besançon - <a href="https://jmb2021.sciencesconf.org/">Forum des Jeunes Mathématicien.ne.s. </a><br>
  <b>Statistical analysis of graph structured data, via heat diffusion processes.</b> - [<a href="slides/2021_12_09_FJM.pdf">slides</a>]<br>
    <button class="showhide abstract4" onclick="switchDisplayByClassName('abstract4')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
    <button class="showhide abstract4" onclick="switchDisplayByClassName('abstract4')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
    <p class="abstracttext abstract4" style="display : none;">
      When working with weighted graphs, one can interpret weights as the thermal conductivity of edges. This means that heat diffuses faster along edges with higher weights. Given initial conditions, one can use the way heat diffuses to compare graphs. But choosing a relevant and informative diffusion time is often essential and challenging. To circumvent this issue, we choose to take into account the whole diffusion process. For that, we define real-valued processes indexed by all the diffusion times in [0, T] for some T > 0, namely the Heat Kernel Distance (HKD) process and the Heat Persistence Distance (HPD) process. Borrowing tools from TDA, the HPD process is able to compare graphs without known node correspondence or even graphs with different sizes. In this talk, I will introduce these processes and present their statistical properties. Namely, we proved under mild assumptions that they verify a functional central limit theorem and admit a gaussian approximation. Moreover, I will present potential applications of these processes (the construction of confidence bands and two-sample tests).
  </p>
  </li>
<br>
    <li>Oct, 2021 - île d'Oléron - <a href="https://jps-2021.sciencesconf.org/">Colloque Jeunes Probabilistes et Statisticiens. </a> <br>
  <b>Statistical analysis of graph structured data, via heat diffusion processes.</b> - [<a href="slides/2021_10_26_CJPS.pdf">slides</a>]<br>
    <button class="showhide abstract3" onclick="switchDisplayByClassName('abstract3')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
    <button class="showhide abstract3" onclick="switchDisplayByClassName('abstract3')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
    <p class="abstracttext abstract3" style="display : none;">
      When working with weighted graphs, one can interpret weights as the thermal conductivity of edges. This means that heat diffuses faster along edges with higher weights. Given initial conditions, one can use the way heat diffuses to compare graphs. But choosing a relevant and informative diffusion time is often essential and challenging. To circumvent this issue, we choose to take into account the whole diffusion process. For that, we define real-valued processes indexed by all the diffusion times in [0, T] for some T > 0, namely the Heat Kernel Distance (HKD) process and the Heat Persistence Distance (HPD) process. Borrowing tools from TDA, the HPD process is able to compare graphs without known node correspondence or even graphs with different sizes. In this talk, I will introduce these processes and present their statistical properties. Namely, we proved under mild assumptions that they verify a functional central limit theorem and admit a gaussian approximation. Moreover, I will present potential applications of these processes (the construction of confidence bands and two-sample tests).
    </p>
  </li>
<br>
    <li>Oct, 2021 - Orsay - Working group of the Probability-Statistics team.<br>
  Presentation of <b>Finding Adam in random growing trees</b> [<a href="https://doi.org/10.1002/rsa.20649">article</a>].<br>
  <button class="showhide abstract2.5" onclick="switchDisplayByClassName('abstract2.5')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
  <button class="showhide abstract2.5" onclick="switchDisplayByClassName('abstract2.5')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
  <p class="abstracttext abstract2.5" style="display : none;">
  The article deals with the estimation of the root when observing the structure of a large tree following random recursive tree models : either the uniform attachment model or the preferential attachment model. I only presented the results for the uniform attachment model.
  The weaker result shows that the root is recovered with probability greater than 1-&#949, when the set of candidates for the root is of polynomial size in 1/&#949, where the candidates are chosen as the ones whose largest subtree is the smallest.
  The strongest result shows that set of candidates can be chosen with subpolynomial size, by looking at a more refined statistic. I presented both results and the proof of the weaker one.
</li>
<br>
    <li>Oct, 2021 - U. Paris-Saclay - <a href="https://team.inria.fr/datashape/seminars/"">Datashape seminar</a> <br>
    <b>Heat diffusion distance processes: a statistically founded method to analyze graph data sets.</b> - [<a href="slides/2021_10_07_datashape_seminar.pdf">slides</a>]<br>
      <button class="showhide abstract2" onclick="switchDisplayByClassName('abstract2')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
      <button class="showhide abstract2" onclick="switchDisplayByClassName('abstract2')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
      <p class="abstracttext abstract2" style="display : none;">
        When working with weighted graphs, one can interpret weights as the thermal conductivity of edges. This means that heat diffuses faster along edges with higher weights. Given initial conditions, one can use the way heat diffuses to compare graphs. But choosing a relevant and informative diffusion time is often essential and challenging. To circumvent this issue, we choose to take into account the whole diffusion process. For that, we define real-valued processes indexed by all the diffusion times in [0, T] for some T > 0, namely the Heat Kernel Distance (HKD) process and the Heat Persistence Distance (HPD) process. Borrowing tools from TDA, the HPD process is able to compare graphs without known node correspondence or even graphs with different sizes. In this talk, I will introduce these processes and present their statistical properties. Namely, we proved under mild assumptions that they verify a functional central limit theorem and admit a gaussian approximation. Moreover, I will present potential applications of these processes (the construction of confidence bands and two-sample tests, the study of neural networks through their activation graphs).
      </p>
    </li>
  <br>
    <li>Mar, 2021 - U. Paris-Saclay - <a href="https://www.imo.universite-paris-saclay.fr/en/activities/seminars-events/">vulgarization seminar</a>  <br>
    <b>Gaussian approximations for random functions.</b> - [<a href="slides/2021_03_17_semdoct.pdf">slides</a>]<br>
      <button class="showhide abstract1" onclick="switchDisplayByClassName('abstract1')" style="display: block;"><div class="arrow-right"></div>Show Abstract<span class="dots">...</span></button>
      <button class="showhide abstract1" onclick="switchDisplayByClassName('abstract1')" style="display : none;"><div class="arrow-top"></div>Hide Abstract</button>
      <p class="abstracttext abstract1" style="display : none;">
        The Central Limit Theorem indicates that a properly rescaled sum of random variables converges in distribution to a gaussian distribution.
        This is where the gaussian approximation problem arises :
        is it possible to draw these random variables, as well as a gaussian variable, such that the rescaled sum and the gaussian variable are close? <br>
        The presentation starts by a few reminders concerning standard probability results : law of large numbers, central limit theorem, Berry-Essen theorem, and quantile transformation.
        Then, historical results on gaussian approximation are presented.
        It includes, for the real case, the Skorokhod embedding and the Komlos Major and Tusnady (KMT) approach,
        as well as Zaitsev's results for the multidimensional case and Koltchinskii's results for general empirical processes.
        The end of this presentation concerns my thesis subject and its link with the gaussian approximation problem for random functions.
      </p>
    </li>
  </ul>

  <h2 id="material" class="title2">Material</h2>
  <ul class="normalsize">
    <li> My Ph.D. thesis is entitled <b><i>Contributions to statistical analysis of graph-structured data</i></b>. <br>
      You can find the manuscript and the slides of the defense here : [<a href="material/these.pdf">manuscript</a>] [<a href="material/2022_12_05_soutenance.pdf">slides</a>].
    </li>
  </ul>

  <h2 id="others" class="title2">Others</h2>
  <h3 class="title2"> Editorial activities</h3>
  <ul class="normalsize">
    <li>Reviewing activity for the <a href="https://bernoullisociety.org/publications/bernoulli-journal">Bernoulli Journal</a>.</li>
    <li>May 2023 - Review of a communication proposal for the 2023 edition of the <a href="http://gretsi.fr/">GRETSI</a> conference,<br>
        Special session: <i>Graph Learning and Learning with Graphs.</i></li>
  </ul>
  <h3 class="title2">Awards</h3>
  <ul class="normalsize">
    <li> Winner of the challenge : <a href="https://challenge-maths.sciencesconf.org/"> Mathématiques et Entreprises </a> 2021, with Olympio Hacquart and <a href="https://vadimlebovici.github.io/">Vadim Lebovici</a>.
      <button class="showhide challenge" onclick="switchDisplayByClassName('challenge')" style="display: block;"><div class="arrow-right"></div>More info<span class="dots">...</span></button>
      <button class="showhide challenge" onclick="switchDisplayByClassName('challenge')" style="display : none;"><div class="arrow-top"></div>Hide</button>
      <p class="abstracttext challenge" style="display : none;">
        Subject proposed by french company <a href="https://eurecam.net/en/">Eurecam</a> : Reconstruction of trajectories from real life 3D detections of people.
        We choose to use optimal transport with boundary to reconstruct trajectories frame by frame. We proposed an ad hoc minimization problem.
        Pre-processing and post-processing steps were implemented to obtain better reconstructions.
        Our work can be found <a href="https://github.com/elasalle/challengeMathEntreprises">here</a>.
      </p>
    </li>
  </ul>
</div><!-- /.blurb -->
